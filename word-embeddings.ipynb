{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# Word Embeddings\n",
    "\n",
    "## ¬øC√≥mo representamos palabras, oraciones y significados en NLP?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## ¬øQu√© es una palabra?\n",
    "\n",
    "\n",
    "![](img/words.png)\n",
    "\n",
    "Cuando hablamos de palabras, podemos distinguir dos conceptos diferentes:\n",
    "\n",
    "- **ocurrencia** (*token*) se refiere a una observaci√≥n de una palabra en una cadena de texto. \n",
    "\n",
    "    Como hemos visto, en algunas lenguas es m√°s o menos complejo identificar los l√≠mites de las palabras, pero en la mayor√≠a de las lenguas occidentales y de nuestro entorno se utilizan espacios y otros signos de puntuaci√≥n para delimitar las palabras.\n",
    "\n",
    "- **tipo** (*type*) es la representaci√≥n abstracta de una palabra. Cad **ocurrencia** pertenece a un **tipo** de palabra. Cuando contamos la frecuencia de las palabras de un *corpus* o colecci√≥n de textos, lo que hacemos es contar el n√∫mero de ocurrencias que tiene cada tipo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "\n",
    "texts = [\n",
    "    \"\"\"No hubo sorpresa en Bruselas. 621 votos a favor, 49 en contra (los 'remainers' brit√°nicos entre ellos) y 13 abstenciones.\"\"\",\n",
    "    \"\"\"'The Fulton County Grand Jury said Friday an investigation of Atlanta's recent primary election produced \"no evidence\" that any irregularities took place.'\"\"\",\n",
    "    \"\"\"Áí∞Â§™Âπ≥Ê¥ãÈÄ†Â±±Â∏Ø„Å´Â±û„Åô„ÇãÂ∞è„Çπ„É≥„ÉÄÂàóÂ≥∂„ÅÆË•øÁ´Ø„Å´‰ΩçÁΩÆ„Åó„Å¶„ÅÑ„Çã„ÄÇ\"\"\",\n",
    "]\n",
    "\n",
    "for text in texts:\n",
    "    print(word_tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "tweets = [\n",
    "    \"\"\"üéâ¬°#SORTEO! Gana una tostadora YummyToast Double. üéÅ \n",
    "‚ñ™Ô∏èS√≠guenos. \n",
    "‚ñ™Ô∏èComenta mencionando a 2 amigos junto a #Cecotec.\n",
    "Tienes hasta el 9 de febrero para participar. El regalo se sortear√° aleatoriamente entre los participantes. ¬°Mucha suerte!.\"\"\",\n",
    "    \"\"\"we play for y‚Äôall üèÄ‚ÄºÔ∏èüñ§ https://t.co/sd12vW93 #MambaMentality\"\"\",\n",
    "]\n",
    "\n",
    "for tweet in tweets:\n",
    "    print(word_tokenize(tweet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "tokenizer = TweetTokenizer()\n",
    "\n",
    "for text in texts:\n",
    "    print(tokenizer.tokenize(text))\n",
    "\n",
    "for tweet in tweets:\n",
    "    print(tokenizer.tokenize(tweet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "for text in texts + tweets:\n",
    "    doc = nlp(text)\n",
    "    tokens = [token.text for token in doc]\n",
    "    print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Veamos qu√© tipo de tokenizaci√≥n se prefiere cuando son humanos los que segmentan las palabras: el [corpus de Brown](https://en.wikipedia.org/wiki/Brown_Corpus) en ingl√©s, o [Ancora](http://clic.ub.edu/corpus/es) en espa√±ol."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import brown\n",
    "\n",
    "brown_sents = brown.tagged_sents(categories=\"news\")\n",
    "for sentence in brown_sents[:3]:\n",
    "    print([token for token, _tag in sentence])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import cess_esp\n",
    "\n",
    "ancora_sents = cess_esp.tagged_sents()\n",
    "for sentence in ancora_sents[:3]:\n",
    "    print([token for token, _tag in sentence])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Representaciones discretas\n",
    "\n",
    "A partir de aqu√≠ vamos a asumir que tenemos solucionado el proceso de tokenizaci√≥n e identificaci√≥n de lo que es una palabra. ¬øC√≥mo continuamos?\n",
    "\n",
    "La manera m√°s sencilla de representar una palabra es una cadena, es decir, como una secuencia ordenada de caracteres. Esto es c√≥modo, pero implica dos cosas:\n",
    "\n",
    "- La cantidad de memoria que ocupa cada cada palabra var√≠a en funci√≥n de la longitud :-/\n",
    "\n",
    "- Comprobar si dos palabras son id√©nticas es un proceso lento :-(\n",
    "\n",
    "Otra opci√≥n alternativa consiste en representar las palabras como n√∫meros enteros, de manera que a cada palabra se le asigna de manera m√°s o menos arbitraria un n√∫mero entero positivo.\n",
    "\n",
    "- Todos las palabras ocupan la cantidad de memoria.\n",
    "\n",
    "- Comprobar si dos cadenas contienen las mismas palabras es r√°pido :-)\n",
    "\n",
    "- Estos identificadores arbitrarios no significan nada :-(\n",
    "\n",
    "- No hay manera de relacionar palabras similares atendiendo a su identificador :-(\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Palabras como vectores distribucionales\n",
    "\n",
    "> ‚ÄúYou shall know a word by the company it keeps.‚Äù\n",
    "> ‚Äî John R. Firth (1957)\n",
    ">\n",
    ">‚ÄúThe meaning of a word is its use in the language (‚Ä¶) One cannot guess how a word functions. One has to look at its use, and learn from that.‚Äù\n",
    ">‚Äî Ludwig Wittgenstein (1953)\n",
    "\n",
    "\n",
    "La idea de que podemos analizar el uso de las palabras para deducir sus significado es una idea fundamental en sem√°ntica distribucional: la hip√≥tesis distribucional. \n",
    "\n",
    "Esta idea inspira muchos algoritmos para aprender repesentaciones num√©ricas de las palanbras --> *word embeddings*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "categories = [\"comp.windows.x\", \"rec.sport.baseball\", \"sci.space\", \"talk.religion.misc\"]\n",
    "remove = (\"headers\", \"footers\", \"quotes\")\n",
    "\n",
    "newsgroups_train = fetch_20newsgroups(\n",
    "    subset=\"train\", categories=categories, remove=remove\n",
    ")\n",
    "\n",
    "newsgroups_train.filenames.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "for doc in newsgroups_train.data[:3]:\n",
    "    print(doc[:300])\n",
    "    print(\"-\" * 100)\n",
    "\n",
    "for doc in newsgroups_train.data[-3:]:\n",
    "    print(doc[:300])\n",
    "    print(\"-\" * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "tweets_tokens = []\n",
    "tweets_tokens.extend([word_tokenize(tweet) for tweet in tweets][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "\n",
    "tokens_int = label_encoder.fit_transform(tweets_tokens)\n",
    "\n",
    "token2int = dict(zip(tweets_tokens, tokens_int))\n",
    "print(token2int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "onehot_encoder = OneHotEncoder(sparse=False)\n",
    "\n",
    "# aplanamos los tokens_int\n",
    "tokens_int = tokens_int.reshape(len(tokens_int), 1)\n",
    "onehot_tokens = onehot_encoder.fit_transform(tokens_int)\n",
    "\n",
    "print(onehot_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "\n",
    "onehot_tokens = to_categorical(tokens_int)\n",
    "\n",
    "print(onehot_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# invert first example\n",
    "inverted = label_encoder.inverse_transform([np.argmax(onehot_encoded[0, :])])\n",
    "print(inverted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(stop_words=\"english\")\n",
    "\n",
    "vectors = vectorizer.fit_transform(\n",
    "    newsgroups_train.data\n",
    ").todense()  # (documents, vocab)\n",
    "\n",
    "vectors.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Hemos convertido la collecci√≥n de documentos en una matriz de datos donde los documentos son vectores de enteros.\n",
    "\n",
    "![](img/vectorized-docs.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "vectors[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "vocab = np.array(vectorizer.get_feature_names())\n",
    "vocab.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "print(vocab[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "print(vocab[20000:20050])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "print(vocab[-50:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Palabras como vectores\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
