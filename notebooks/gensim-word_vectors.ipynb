{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# Ejemplo de `word2vec` y `fastText` con `gensim`\n",
    "\n",
    "\n",
    "En la siguiente celda, importamos las librerías necesarias y configuramos los mensajes de los logs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import gensim, logging, os\n",
    "\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s : %(levelname)s : %(message)s\", level=logging.INFO\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## word2vec \n",
    "\n",
    "He entrenado un modelo de word2vec con la Wikipedia en español, [tal y como explico aquí](https://github.com/vitojph/kschool-nlp-16/tree/master/misc)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-02-08 15:07:12,105 : INFO : loading Word2Vec object from ../data/eswiki-300.w2v\n",
      "2020-02-08 15:07:14,092 : INFO : loading trainables recursively from ../data/eswiki-300.w2v.trainables.* with mmap=None\n",
      "2020-02-08 15:07:14,094 : INFO : loading syn1neg from ../data/eswiki-300.w2v.trainables.syn1neg.npy with mmap=None\n",
      "2020-02-08 15:07:14,449 : INFO : loading vocabulary recursively from ../data/eswiki-300.w2v.vocabulary.* with mmap=None\n",
      "2020-02-08 15:07:14,450 : INFO : loading wv recursively from ../data/eswiki-300.w2v.wv.* with mmap=None\n",
      "2020-02-08 15:07:14,451 : INFO : loading vectors from ../data/eswiki-300.w2v.wv.vectors.npy with mmap=None\n",
      "2020-02-08 15:07:14,787 : INFO : setting ignored attribute vectors_norm to None\n",
      "2020-02-08 15:07:14,788 : INFO : setting ignored attribute cum_table to None\n",
      "2020-02-08 15:07:14,789 : INFO : loaded ../data/eswiki-300.w2v\n"
     ]
    }
   ],
   "source": [
    "MODEL = \"../data/eswiki-300.w2v\"\n",
    "model = gensim.models.Word2Vec.load(MODEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Probando nuestro modelo\n",
    "\n",
    "El objeto `model` contiene una enorme matriz de números: una tabla, donde cada fila es uno de los términos del vocabulario reconocido y cada columna es una de las características que permiten modelar el significado de dicho término.\n",
    "\n",
    "En nuestro modelo, tal y como está entrenado, tenemos más de 41 millones de términos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41843901\n"
     ]
    }
   ],
   "source": [
    "print(model.corpus_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Cada término del vocabulario está representado como un vector con 300 dimensiones. Podemos acceder al vector de un término concreto:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 6.97230875e-01 -2.16513014e+00  3.48829389e-01 -9.89584625e-01\n",
      " -1.02388668e+00 -2.17564240e-01 -1.96011472e+00  2.94649076e+00\n",
      " -9.12335694e-01 -6.84771001e-01  1.52577829e+00  7.75085762e-02\n",
      " -7.02408105e-02  4.89242375e-01  1.37050912e-01 -9.30723965e-01\n",
      "  6.80433512e-02 -1.08530678e-01  2.31580353e+00 -5.53485096e-01\n",
      " -3.57593626e-01 -2.28935838e+00 -1.50510311e-01  1.26760174e-02\n",
      "  6.07259810e-01 -2.17627048e+00  2.73906016e+00  1.09757566e+00\n",
      " -1.71337926e+00  1.64041713e-01  6.90737784e-01  1.26919913e+00\n",
      " -5.12029469e-01 -2.01885894e-01  1.78642869e-01  1.53002536e+00\n",
      " -1.43445313e-01  1.11213720e+00 -1.41976345e+00 -1.72563398e+00\n",
      "  7.15158105e-01  1.11852288e+00 -5.18750012e-01 -7.85638809e-01\n",
      " -2.75143218e+00 -2.21702838e+00 -7.32014835e-01 -4.00392205e-01\n",
      "  1.58708438e-01  1.02694380e+00 -2.80862898e-01  1.41663051e+00\n",
      "  1.11722755e+00 -1.24235370e-03 -1.14149344e+00 -1.97176194e+00\n",
      " -2.22860837e+00 -1.23289347e+00  1.74586341e-01  4.37671602e-01\n",
      " -9.38071907e-01  4.78671581e-01  7.36256719e-01 -7.83176646e-02\n",
      " -1.51940608e+00 -6.11350536e-01  5.75296767e-02 -2.24569416e+00\n",
      "  2.29354954e+00  1.99835587e+00 -2.03271103e+00 -2.48154378e+00\n",
      " -3.93771321e-01 -1.42419696e+00 -7.79238284e-01  3.22412670e-01\n",
      " -1.73642015e+00  5.40155828e-01 -5.78199029e-01  5.51159739e-01\n",
      " -9.22807753e-02  7.58624494e-01 -8.06146041e-02 -5.02858400e-01\n",
      "  2.51609683e-01 -3.62324938e-02 -1.28702569e+00 -8.80274713e-01\n",
      "  6.67085648e-01 -2.49828964e-01 -2.21324468e+00  4.57783714e-02\n",
      "  2.01037121e+00  8.71509194e-01  8.05255115e-01  1.56539035e+00\n",
      " -1.58826590e+00 -1.57694650e+00  1.55030891e-01 -1.33787319e-01\n",
      "  1.50556117e-01 -9.86127973e-01  1.19684458e+00  1.60300064e+00\n",
      "  1.51182365e+00 -6.53138340e-01 -4.16660517e-01  7.05885530e-01\n",
      "  1.72533154e+00  5.92924237e-01  1.47913015e+00 -4.57063913e-01\n",
      "  1.03894782e+00  4.79001194e-01 -3.50626111e-01 -1.77991197e-01\n",
      "  2.19397569e+00 -3.50124621e+00  6.04100525e-01 -5.21477520e-01\n",
      "  1.12570369e+00 -4.08726096e-01  4.44705516e-01  4.09761578e-01\n",
      "  2.13509130e+00  1.38557002e-01  1.62473202e-01 -8.46179843e-01\n",
      "  9.31195915e-01 -3.71493864e+00 -2.28744125e+00  2.43249297e-01\n",
      "  2.68338442e-01 -4.84084219e-01  1.25886393e+00  3.98710579e-01\n",
      "  1.25982332e+00  7.55630493e-01 -7.38484979e-01 -2.37317964e-01\n",
      " -2.37362766e+00 -1.01889896e+00 -1.62790522e-01  2.23520666e-01\n",
      " -7.00491130e-01 -1.96538699e+00 -1.66521466e+00  6.09001160e-01\n",
      "  2.04099081e-02 -5.20849288e-01  1.09548306e+00  2.20493245e+00\n",
      "  2.61340678e-01  3.53619397e-01  8.09817687e-02 -2.95696783e+00\n",
      "  3.11337614e+00  6.74783945e-01  1.36555350e+00  1.23942241e-01\n",
      " -1.88764942e+00  9.64717101e-03 -5.81398785e-01  1.15578747e+00\n",
      " -3.71559173e-01 -1.10587966e+00  1.01144716e-01  7.56232798e-01\n",
      " -1.80072689e+00 -1.13651741e+00  7.86995739e-02 -3.01128364e+00\n",
      " -7.75180817e-01  1.91889703e-01 -5.09170413e-01  1.11552632e+00\n",
      " -1.63304591e+00 -1.33140087e-01 -3.51848173e+00  6.92798123e-02\n",
      " -1.31583095e+00  7.52560794e-01 -6.96992874e-02 -1.38193440e+00\n",
      "  7.89924383e-01  1.55027533e+00 -1.00363612e+00  1.19020355e+00\n",
      "  1.72872794e+00 -8.95594299e-01  4.67942744e-01 -7.27509320e-01\n",
      "  9.13300335e-01 -1.87429321e+00  3.30061197e-01  1.27837038e+00\n",
      " -4.05655235e-01  7.67004073e-01 -8.62265468e-01  3.82711262e-01\n",
      "  1.32297933e+00 -1.83670759e+00 -3.55419707e+00  1.26548871e-01\n",
      " -3.11232537e-01 -5.10343015e-01 -1.39698699e-01 -1.74266481e+00\n",
      " -7.93091238e-01  4.29079247e+00 -5.21301925e-01 -1.87273645e+00\n",
      "  1.39592752e-01  5.60675561e-01  2.89429605e-01  8.56994331e-01\n",
      "  9.63832259e-01 -1.02407742e+00 -1.02430320e+00 -2.36571741e+00\n",
      " -1.87877893e+00 -4.33141023e-01  4.01332825e-02 -7.20952272e-01\n",
      "  1.04298246e+00  1.55859351e+00 -1.65808499e-01 -2.73690522e-01\n",
      " -1.26727343e+00  5.67542493e-01 -4.43337083e-01 -1.87876606e+00\n",
      " -1.47362781e+00 -5.99237025e-01 -6.35434508e-01  1.67620635e+00\n",
      " -2.79128814e+00  4.42338854e-01 -5.47896475e-02 -7.08627760e-01\n",
      "  2.17417821e-01  2.32921076e+00  5.97613633e-01 -9.80228364e-01\n",
      "  5.66773295e-01 -2.75546551e+00  3.42741442e+00 -1.93800759e+00\n",
      " -1.12337448e-01  1.59620181e-01 -1.26223087e+00  1.69620776e+00\n",
      "  1.91346610e+00 -3.90370280e-01 -1.36392450e+00  1.15976274e+00\n",
      "  2.13117814e+00 -6.52291358e-01  1.74113953e+00 -1.69489300e+00\n",
      "  3.71365786e-01 -2.36955002e-01  1.44504333e+00 -1.04361522e+00\n",
      "  7.62583852e-01  1.43805814e+00  2.82243609e+00 -5.56403518e-01\n",
      " -8.44093025e-01  3.30600627e-02 -2.77741647e+00  5.12723386e-01\n",
      " -1.31796634e+00 -8.08147073e-01  1.02912910e-01  1.56859711e-01\n",
      "  3.66381526e-01 -1.72485852e+00  3.80273843e+00  4.15444702e-01\n",
      " -3.00814676e+00 -1.28893280e+00 -6.98973715e-01  1.81277776e+00\n",
      " -8.92491996e-01  2.99211264e-01 -4.96248603e-01  8.53611708e-01\n",
      "  8.95331442e-01 -7.83302605e-01  1.56157017e+00 -3.60860467e-01\n",
      " -1.29427481e+00 -2.57891870e+00  7.82728970e-01 -7.38334477e-01\n",
      " -1.95549154e+00  4.74393606e-01  1.24309886e+00 -1.18519068e+00]\n",
      "[ 1.3850095  -2.5660565   1.3728682  -0.83303195 -2.301186    0.07675269\n",
      " -1.8424745   2.334831    0.7353917  -1.4068612   0.3638355  -1.0050164\n",
      " -0.2690413   1.077528    0.01462558 -1.3692732  -0.78760326 -0.6617992\n",
      "  0.4350986  -0.4756711  -1.9598641  -2.8795297   0.1688058   1.0000659\n",
      "  0.9068427  -2.3099787   1.6753478  -0.592722   -1.230077    0.9378993\n",
      " -1.015693    0.7357098  -1.6021743  -0.7928299   0.4073866   1.0253177\n",
      " -1.2019182   1.2335608  -1.5969017  -2.8415644  -0.10602891  1.4769062\n",
      " -0.41869932 -0.8175206  -1.124196   -0.73385465 -0.76749897 -0.46764067\n",
      "  0.09836564  0.6741108  -0.46414846 -0.4624298  -1.0011635  -0.6140801\n",
      "  0.79694235 -1.2713541  -3.3046262  -0.6018643  -0.4108396   1.3070939\n",
      " -1.8652301  -0.4904522  -0.9339131  -0.02710116 -0.04858475 -0.01863806\n",
      "  0.70001256 -1.6557547   1.5628458   0.5064657  -1.797151   -2.3030412\n",
      " -2.2051547  -1.218277   -0.7427246   0.06319208 -2.5523772   0.4843061\n",
      " -1.993314   -0.33655906 -0.02596497 -0.5703391  -0.43692055  0.9104439\n",
      " -0.47017547  0.80330557 -0.08766358 -0.99301153  1.3783613  -0.5682676\n",
      " -3.879904    0.45949453  1.8900665   0.01243172  1.8783422   1.1829813\n",
      " -2.5922866   0.12871265 -2.2297754  -0.31814438 -1.0467976  -0.46241945\n",
      "  0.48486632  1.3818852   2.2452068  -0.85533184 -2.5036528  -0.495667\n",
      " -0.4324654   1.9849738  -0.13014191  0.06659676  1.7623967   1.0213025\n",
      " -0.5270396  -1.5989958   1.1503649  -0.2123848   0.21236254 -0.7776451\n",
      "  0.84430623 -0.08506089 -0.13721952  1.1191903   0.6019633  -1.6842045\n",
      "  1.2636555  -1.3091834  -0.01886843 -3.2370481  -1.750949    0.10372003\n",
      "  1.5831363  -0.23176104  1.515895   -1.0456054   0.18147561 -0.03656266\n",
      " -0.94207764 -0.3497908  -1.6436495  -0.42169568  0.8620131   0.91871876\n",
      " -0.22054364 -1.0168042  -1.9552135   1.1035328  -0.3596375  -0.23637433\n",
      "  0.5581769   1.6555921   0.390328    0.5012816  -1.7357541  -1.1455238\n",
      "  3.0644748  -0.35107726 -0.18092088  2.06072    -1.044605   -0.75169975\n",
      " -1.348437   -1.1832602  -0.75934815 -0.5912932   1.5305748  -0.31882104\n",
      " -1.7354072  -0.56326985 -0.8324436  -2.7235446  -1.9108884   0.15623206\n",
      " -0.36186224  1.9471594  -1.2625138   0.29412252 -4.964669    0.24443157\n",
      " -0.87357754  1.4708598  -1.1896633  -0.82032025 -0.9750552  -0.49026233\n",
      " -0.95575684 -0.5397434   0.9972759  -1.9036691  -0.4355012  -0.8170868\n",
      " -0.12397185 -0.6530273   0.09309008  0.2912987   0.31036985  0.65924746\n",
      "  0.18066719  2.7069798   0.38605925 -1.0387297  -3.1085994  -0.11119398\n",
      "  1.6518806  -1.2821884   0.8726777  -2.3186212   0.7347062   1.6842291\n",
      " -0.79058546 -2.2915156  -0.27359116  1.4600288  -1.1183183   0.899426\n",
      "  1.0799401   0.54588217 -1.103471   -0.37159386 -1.0858496  -0.39178383\n",
      "  0.55319625  0.2622006   1.4281243   0.69389606  0.41142863  0.5143603\n",
      " -1.6273003   1.1035215  -0.43345305 -0.04946567 -0.23303723  0.9368652\n",
      " -1.3205837   0.08098741 -3.1608548  -0.1615996   0.4714969   0.15656589\n",
      " -0.17669348  1.6654254  -1.0738145   0.55626124  0.8885079  -3.1528292\n",
      "  0.76466614 -2.168133   -1.3192146  -0.917789   -0.9890472   1.3261414\n",
      "  1.7859875  -0.5672534  -0.6837446   0.41443905  2.224871   -1.0765458\n",
      "  1.3547662   0.35469246  0.13902354  1.0994624   2.3588822  -0.99958295\n",
      "  1.0783961   2.1091003   1.8400725  -0.75855494  0.37077293  1.8411133\n",
      " -1.3075514   0.75903344 -1.3629781  -0.48467997 -0.21393992 -0.07952596\n",
      " -0.07383763 -1.3894367   2.998921   -0.4671461  -2.4077444  -1.6638222\n",
      " -2.6744215   0.79063314 -1.5543416   0.07679477  0.3395964   0.14223379\n",
      "  0.35985646 -0.6671326   0.97926563  0.78287697  0.85245377 -0.9965855\n",
      "  0.90767205 -0.5029596  -1.7588158   1.1506425   0.5166969  -1.7216785 ]\n",
      "[ 0.7847085  -1.5329005  -0.46471998  0.02011241 -0.08099692  1.1514796\n",
      " -0.74601275  0.35062486  0.51571995 -1.0043101  -0.5037894   0.53727543\n",
      "  0.19627786  0.9447497  -0.29826918 -0.6932724   1.1230501   0.47570503\n",
      " -0.79155195 -0.28073907 -0.48465055  0.19394119 -0.75852925  0.90110654\n",
      "  0.14112549  0.53743047  0.97223073  0.08786259 -1.3785018  -0.04060126\n",
      " -0.52875257  0.8465847   1.5188636  -0.58389276  0.4140893   0.82434577\n",
      "  1.4376293   0.28926733 -0.91270787  0.08191123  0.647682    1.0053891\n",
      " -0.19499062  0.04332691  0.60312915 -0.54246193  0.17632812 -0.6039053\n",
      "  0.49596444  2.0323894  -1.8387872  -0.05593713 -2.1521075  -1.6264348\n",
      " -0.7728398   1.2905133  -0.6376648  -1.0680786  -0.39206806  0.19146046\n",
      " -1.2023246   0.18629618  0.55487716  0.29768056 -0.48967952 -0.03124499\n",
      " -1.0297039  -0.6659895   0.9284774  -1.6000236  -0.3292721  -0.13056895\n",
      " -0.4155102  -0.8480601  -1.5812019   0.657739   -0.7135351   0.30576843\n",
      "  0.2764437  -0.62048     0.527289    1.6644152   1.4342033  -0.07902558\n",
      " -0.55826336 -0.16604295 -0.82217115  0.64606464 -0.14229941 -0.26670864\n",
      " -2.2758875   2.1237724   1.3689176  -0.08999745  2.023159   -1.476428\n",
      "  0.5118154  -0.6255841  -0.22785571 -0.62434787 -0.08261719 -0.8785305\n",
      " -1.3567322   1.1653037   0.93816775 -0.15281363 -0.32789645  1.6753359\n",
      "  0.62778    -1.0580785  -0.4307302  -0.53269416  0.69324434  0.32829532\n",
      " -0.21369718  1.2528694   0.6693925  -0.7098627   0.80196935  1.1918916\n",
      " -0.8067069  -0.6266483   0.7134139   0.5650169   1.9047611  -1.0664423\n",
      "  0.20404051 -0.6249274   0.38135365  0.7165739   1.075217   -0.7554505\n",
      " -0.53848803 -3.5454085   0.8576208  -1.3416232  -1.0241503  -0.6494619\n",
      " -0.1828006  -0.86892307  0.91186196  1.6674222   0.44647202  0.5700854\n",
      " -0.5233042   1.0535731  -0.45213118  0.2906812  -0.12327691  0.6607178\n",
      "  0.00478246  0.59816885 -0.01517303  0.7002586  -1.1069064  -0.1896621\n",
      "  2.0060377   0.8507431   0.7067776  -1.6610572  -2.0192332   0.38746393\n",
      " -0.6694519   0.26885    -0.59255904  0.7935129   0.94851094 -0.08033819\n",
      " -1.0639771  -1.0850402  -1.4941739  -1.0848876  -1.6934092   0.10582539\n",
      "  1.0986395   1.3041579  -1.246617    0.6127787  -1.6422062  -0.38229188\n",
      " -1.2230775   0.6162084  -0.17509484 -0.7146441   0.74357176 -1.0850772\n",
      "  0.29860896 -2.0885625  -0.2770902  -1.7732935   1.0882903   0.00721864\n",
      " -1.941843   -0.01170586  0.5925648   0.23700812 -0.34625673 -0.18503731\n",
      "  0.09212354  0.6710799  -0.11298622 -0.32499132 -0.9813489  -0.49484372\n",
      "  0.9547403  -0.8036505   0.45426053 -2.2597308  -0.2451897  -0.8782322\n",
      " -0.20828019  0.2641839   0.2847426  -0.33382306 -1.9588499  -0.23237209\n",
      " -0.45237535 -1.0834743   0.20310521 -0.34100407 -1.74072    -2.2997932\n",
      " -0.5996378  -0.9578312   1.174139   -0.7993889   2.521483   -0.36843815\n",
      "  0.4483839   0.5298028   1.6750953  -1.2480273  -1.1704851   0.07223753\n",
      "  0.54444003 -0.51446956 -0.33506566 -0.9388432  -0.06317332 -0.3221516\n",
      "  0.2744224   2.378445   -1.1942822   0.17955688  1.6567327   0.01707307\n",
      "  0.22521047  0.3443578   0.1415958  -0.3613423  -1.4395661   0.3148655\n",
      "  0.52782387 -0.5963257   0.408687    1.7245253   1.93999    -1.158203\n",
      " -0.3070527  -0.953129   -0.3346979  -0.05269185 -0.88623625 -1.8645351\n",
      "  0.7703767   0.2729543   2.4485416  -0.4574004   0.11739592 -0.22262135\n",
      "  0.20524342  0.28920907  1.2514988  -1.5343736   0.46565738 -0.6781455\n",
      "  0.6254025   0.3958242   0.12523335  0.04213862 -0.5174048  -1.8053024\n",
      " -0.9706126  -0.5657299   0.8370249   0.0851815   0.1262318   1.0044695\n",
      "  0.57685393  0.31541646  0.7507542  -1.3563049   0.7792753   0.03093588\n",
      "  0.60371226 -0.21577448  0.1682348   0.49394295 -0.94681364 -0.41170546]\n"
     ]
    }
   ],
   "source": [
    "print(model.wv[\"azul\"])\n",
    "print(model.wv[\"verde\"])\n",
    "print(model.wv[\"clorofila\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Estos vectores no nos dicen mucho, salvo que contienen números muy pequeños :-/\n",
    "\n",
    "El mismo objeto `model` permite acceder a una serie de funcionalidades ya implementadas que nos van a permitir evaluar formal e informalmente el modelo. Por el momento, nos contentamos con los segundo: vamos a revisar visualmente los significados que nuestro modelo ha aprendido por su cuenta. \n",
    "\n",
    "Podemos calcular la similitud semántica entre dos términos usando el método `similarity`, que nos devuelve un número entre 0 y 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hombre - mujer 0.43087164\n",
      "perro - gato 0.8005251\n",
      "gato - periódico 0.17121994\n",
      "febrero - azul -0.021135561\n"
     ]
    }
   ],
   "source": [
    "print(\"hombre - mujer\", model.wv.similarity(\"hombre\", \"mujer\"))\n",
    "\n",
    "print(\"perro - gato\", model.wv.similarity(\"perro\", \"gato\"))\n",
    "\n",
    "print(\"gato - periódico\", model.wv.similarity(\"gato\", \"periódico\"))\n",
    "\n",
    "print(\"febrero - azul\", model.wv.similarity(\"febrero\", \"azul\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Podemos seleccionar el término que no encaja a partir de una determinada lista de términos usando el método `doesnt_match`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-02-08 15:07:16,601 : INFO : precomputing L2-norms of word weight vectors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "en la lista 'madrid barcelona gonzález washington' sobra 'washington'\n",
      "en la lista 'psoe pp ciu ronaldo' sobra 'ronaldo'\n",
      "en la lista 'publicaron declararon soy fueron negaron' sobra 'soy'\n",
      "en la lista 'homero saturno cervantes shakespeare cela' sobra 'saturno'\n",
      "en la lista 'madrid barcelona alpedrete marsella' sobra 'alpedrete'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/victor/.conda/envs/nlp/lib/python3.6/site-packages/gensim/models/keyedvectors.py:877: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
      "  vectors = vstack(self.word_vec(word, use_norm=True) for word in used_words).astype(REAL)\n"
     ]
    }
   ],
   "source": [
    "lista1 = \"madrid barcelona gonzález washington\".split()\n",
    "print(f\"\"\"en la lista '{\" \".join(lista1)}' sobra '{model.wv.doesnt_match(lista1)}'\"\"\")\n",
    "\n",
    "lista2 = \"psoe pp ciu ronaldo\".split()\n",
    "print(f\"\"\"en la lista '{\" \".join(lista2)}' sobra '{model.wv.doesnt_match(lista2)}'\"\"\")\n",
    "\n",
    "lista3 = \"publicaron declararon soy fueron negaron\".split()\n",
    "print(f\"\"\"en la lista '{\" \".join(lista3)}' sobra '{model.wv.doesnt_match(lista3)}'\"\"\")\n",
    "\n",
    "lista4 = \"homero saturno cervantes shakespeare cela\".split()\n",
    "print(f\"\"\"en la lista '{\" \".join(lista4)}' sobra '{model.wv.doesnt_match(lista4)}'\"\"\")\n",
    "\n",
    "lista5 = \"madrid barcelona alpedrete marsella\".split()\n",
    "print(f\"\"\"en la lista '{\" \".join(lista5)}' sobra '{model.wv.doesnt_match(lista5)}'\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Podemos buscar los términos más similares usando el método `most_similar` de nuestro modelo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "azul ==> [('verde', 0.7170504331588745), ('amarillo', 0.7170424461364746), ('rojo', 0.7002288103103638), ('azúl', 0.6983712315559387), ('turquesa', 0.6890928745269775), ('morado', 0.6765597462654114), ('carmesí', 0.6581361889839172), ('granate', 0.6548363566398621), ('naranja', 0.6507274508476257), ('beige', 0.6397430896759033)]\n",
      "\n",
      "madrid ==> [('valladolid', 0.7847391366958618), ('barcelona', 0.775865912437439), ('sevilla', 0.760832667350769), ('zaragoza', 0.7517766952514648), ('madrid.', 0.7219818830490112), ('valencia', 0.6873288154602051), ('oviedo', 0.6831979751586914), ('salamanca', 0.6781078577041626), ('málaga', 0.675739049911499), ('pamplona', 0.6748294830322266)]\n",
      "\n",
      "bmw ==> [('audi', 0.7591230869293213), ('porsche', 0.7583965063095093), ('mercedes-benz', 0.7572087645530701), ('renault', 0.7485953569412231), ('toyota', 0.7476786375045776), ('opel', 0.739383339881897), ('volkswagen', 0.7309398055076599), ('maserati', 0.7237600088119507), ('gt', 0.7185136675834656), ('volvo', 0.7153291702270508)]\n",
      "\n",
      "bici ==> [('bicicleta', 0.644951343536377), ('bus-vao', 0.6296814680099487), ('carril-bici', 0.6122264862060547), ('peatonales', 0.5526739358901978), ('peatones', 0.5314311981201172), ('btt', 0.5252747535705566), ('bicicletas', 0.5034662485122681), ('senderismo', 0.4999716877937317), ('asfaltado', 0.498836874961853), ('bicisendas', 0.49882516264915466)]\n",
      "\n",
      "2019 ==> [('2018', 0.9702128171920776), ('2017', 0.9523733258247375), ('2015', 0.9346306920051575), ('2016', 0.9337858557701111), ('2014', 0.9235639572143555), ('2013', 0.9065483808517456), ('2012', 0.8908951282501221), ('2011', 0.8705427646636963), ('2017.', 0.8438525199890137), ('2009', 0.8382002711296082)]\n",
      "\n",
      "rock ==> [('pop', 0.7419593930244446), ('rock/metal', 0.711592435836792), ('reggae', 0.7007744312286377), ('pop-rock', 0.6985785365104675), ('pop/rock', 0.6961304545402527), ('post-punk', 0.6939693689346313), ('hip-hop', 0.6869392991065979), ('rock.', 0.6786236763000488), ('grunge', 0.6774565577507019), ('folk', 0.6747815012931824)]\n",
      "\n",
      "google ==> [('google.', 0.5854734778404236), ('gmail', 0.5553345680236816), ('msn', 0.5442721843719482), ('adsense', 0.5371055006980896), ('hotmail', 0.5290433168411255), ('github', 0.5272660255432129), ('microsoft', 0.5247926712036133), ('icloud', 0.5175305604934692), ('flickr', 0.5160022377967834), ('dropbox', 0.5152624249458313)]\n",
      "\n",
      "shakira ==> [('thalía', 0.7569534778594971), ('beyoncé', 0.7555899620056152), ('rihanna', 0.7475430965423584), ('kesha', 0.7225640416145325), ('fergie', 0.6967421770095825), ('gaga', 0.6915592551231384), ('spears', 0.6893750429153442), ('maluma', 0.6876188516616821), ('will.i.am', 0.6813927888870239), ('pitbull', 0.6813416481018066)]\n",
      "\n",
      "jay-z ==> [('eminem', 0.8365203142166138), ('akon', 0.8173916339874268), ('timbaland', 0.8172226548194885), ('ludacris', 0.8091516494750977), ('ne-yo', 0.802065372467041), ('will.i.am', 0.7991659641265869), ('t-pain', 0.7732182145118713), ('skrillex', 0.7612255811691284), ('rihanna', 0.7609673738479614), ('pitbull', 0.7591802477836609)]\n",
      "\n",
      "xiaomi ==> [('redmi', 0.6870156526565552), ('meizu', 0.6065692901611328), ('lenovo', 0.6043944358825684), ('htc', 0.59620201587677), ('anđeli', 0.5961319208145142), ('smartphone', 0.5951942205429077), ('huawei', 0.591882586479187), ('netbook', 0.5870574712753296), ('rokr', 0.5802743434906006), ('razr', 0.5755516290664673)]\n",
      "\n",
      "rajoy ==> [('rajoy.', 0.6854042410850525), ('aznar', 0.5685055255889893), ('puigdemont', 0.5655163526535034), ('upyd', 0.5523564219474792), ('azaña', 0.5498855710029602), ('rubalcaba', 0.5483195185661316), ('alfonsín', 0.5384674072265625), ('cospedal', 0.5364481806755066), ('ciu', 0.5314044952392578), ('zapatero', 0.5152286887168884)]\n",
      "\n",
      "brexit ==> [('referendum', 0.5395663976669312), ('referendo', 0.529761552810669), ('1-o', 0.5276222229003906), ('referéndum', 0.5275889039039612), ('impeachment', 0.520057201385498), ('aplazamiento', 0.49611973762512207), ('pse', 0.4877656102180481), ('9-n', 0.4866859018802643), ('día-m', 0.48132288455963135), ('ilp', 0.48044028878211975)]\n",
      "\n",
      "saturno ==> [('júpiter', 0.7902184724807739), ('urano', 0.7794459462165833), ('plutón', 0.6895335912704468), ('encélado', 0.6634633541107178), ('neptuno', 0.6610158681869507), ('ío', 0.6490520238876343), ('mimas', 0.6411848068237305), ('marte', 0.638644278049469), ('fobos', 0.6171464920043945), ('deimos', 0.6154087781906128)]\n",
      "\n",
      "césar ==> [('aníbal', 0.6384434103965759), ('cesar', 0.6348336935043335), ('octavio', 0.6310065984725952), ('césar.', 0.6282349824905396), ('nombela', 0.6270837783813477), ('cortázar', 0.6233247518539429), ('cejador', 0.6140755414962769), ('aróstegui', 0.6089369654655457), ('milostich', 0.6085711717605591), ('medem', 0.6021093130111694)]\n",
      "\n",
      "lazio ==> [('fiorentina', 0.7765034437179565), ('sampdoria', 0.7287291884422302), ('udinese', 0.7241649031639099), ('empoli', 0.7175393104553223), ('reggina', 0.7090951204299927), ('frosinone', 0.692754864692688), ('sassuolo', 0.6858640909194946), ('juventus', 0.6848293542861938), ('juve', 0.6831690073013306), ('virtus', 0.6759966015815735)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "terminos = \"azul madrid bmw bici 2019 rock google shakira jay-z xiaomi rajoy brexit saturno césar lazio\".split()\n",
    "\n",
    "for t in terminos:\n",
    "    print(f\"{t} ==> {model.wv.most_similar(t)}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Con el mismo método `most_similar` podemos combinar vectores de palabras tratando de jugar con los rasgos semánticos de cada una de ellas para descubrir nuevas relaciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mujer que ejerce la autoridad en una alcaldía ==> alcalde + mujer - hombre\n",
      "('alcaldesa', 0.7252962589263916)\n",
      "('regidora', 0.6073892116546631)\n",
      "('concejala', 0.5990710258483887)\n",
      "mujer especializada en alguna terapia de la medicina ==> doctor + mujer - hombre\n",
      "('doctora', 0.6626786589622498)\n",
      "('enfermera', 0.5988627672195435)\n",
      "('esposa', 0.5201101303100586)\n",
      "monarca soberano ==> reina + hombre - mujer\n",
      "('rey', 0.6301776170730591)\n",
      "('monarca', 0.5567039251327515)\n",
      "('príncipe', 0.5296465158462524)\n",
      "capital de Alemania ==> moscú + alemania - rusia\n",
      "('berlín', 0.7966121435165405)\n",
      "('múnich', 0.7589867115020752)\n",
      "('hamburgo', 0.7321966886520386)\n",
      "presidente de Francia ==> obama + francia - eeuu\n",
      "('hollande', 0.4829712212085724)\n",
      "('papado', 0.45290911197662354)\n",
      "('napoleón', 0.4527309238910675)\n"
     ]
    }
   ],
   "source": [
    "print(\"mujer que ejerce la autoridad en una alcaldía ==> alcalde + mujer - hombre\")\n",
    "most_similar = model.wv.most_similar(\n",
    "    positive=[\"alcalde\", \"mujer\"], negative=[\"hombre\"], topn=3\n",
    ")\n",
    "for item in most_similar:\n",
    "    print(item)\n",
    "\n",
    "print(\n",
    "    \"mujer especializada en alguna terapia de la medicina ==> doctor + mujer - hombre\"\n",
    ")\n",
    "most_similar = model.wv.most_similar(\n",
    "    positive=[\"doctor\", \"mujer\"], negative=[\"hombre\"], topn=3\n",
    ")\n",
    "for item in most_similar:\n",
    "    print(item)\n",
    "\n",
    "print(\"monarca soberano ==> reina + hombre - mujer\")\n",
    "most_similar = model.wv.most_similar(\n",
    "    positive=[\"reina\", \"hombre\"], negative=[\"mujer\"], topn=3\n",
    ")\n",
    "for item in most_similar:\n",
    "    print(item)\n",
    "\n",
    "print(\"capital de Alemania ==> moscú + alemania - rusia\")\n",
    "most_similar = model.wv.most_similar(\n",
    "    positive=[\"moscú\", \"alemania\"], negative=[\"rusia\"], topn=3\n",
    ")\n",
    "for item in most_similar:\n",
    "    print(item)\n",
    "\n",
    "print(\"presidente de Francia ==> obama + francia - eeuu\")\n",
    "most_similar = model.wv.most_similar(\n",
    "    positive=[\"obama\", \"francia\"], negative=[\"eeuu\"], topn=3\n",
    ")\n",
    "for item in most_similar:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## fastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-02-08 15:10:07,207 : INFO : loading projection weights from ../data/cc.en.300.vec\n",
      "2020-02-08 15:18:04,090 : INFO : loaded (2000000, 300) matrix from ../data/cc.en.300.vec\n"
     ]
    }
   ],
   "source": [
    "from gensim.models.keyedvectors import Word2VecKeyedVectors\n",
    "\n",
    "MODEL = \"../data/cc.en.300.vec\"\n",
    "fasttext = Word2VecKeyedVectors.load_word2vec_format(MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000000\n"
     ]
    }
   ],
   "source": [
    "print(len(fasttext.vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.0271 -0.0098  0.0774  0.0583  0.0299 -0.0128  0.0549  0.02    0.0832\n",
      " -0.0304  0.0518 -0.0156  0.1136  0.0578 -0.0315  0.0225  0.0616 -0.0601\n",
      "  0.0368 -0.002   0.0005 -0.0073  0.0436  0.0322  0.0299  0.0709 -0.0234\n",
      "  0.095  -0.0121  0.1088  0.0547 -0.0087 -0.0978 -0.0141  0.0333 -0.0488\n",
      " -0.0365  0.0512  0.0976  0.0157  0.0037 -0.0768 -0.0619 -0.023  -0.001\n",
      "  0.083  -0.0548  0.0709 -0.0049  0.0537 -0.0042 -0.0442  0.0676  0.0115\n",
      "  0.0061 -0.0051 -0.0135  0.0432 -0.0854 -0.0006 -0.038  -0.0077 -0.0222\n",
      "  0.014  -0.0092  0.1031 -0.0114 -0.0827 -0.0452  0.0223 -0.0192 -0.0613\n",
      " -0.0724  0.0005 -0.0893 -0.0369  0.0982  0.0778 -0.0409 -0.0266 -0.0198\n",
      "  0.0172 -0.0789  0.0652  0.0136  0.0716  0.0195  0.0466  0.087  -0.0316\n",
      " -0.054   0.0148  0.081   0.0166  0.0424 -0.0067  0.0104  0.1335 -0.0011\n",
      " -0.0684  0.0147  0.0002 -0.0033  0.0682  0.0658 -0.046  -0.0615  0.0014\n",
      "  0.0343 -0.0132 -0.0083  0.0179  0.0359 -0.0129 -0.0319 -0.0939 -0.0418\n",
      "  0.0137  0.0129  0.0417 -0.0201  0.0922  0.0622  0.06    0.0626  0.0372\n",
      " -0.0072 -0.0488 -0.0596  0.0137 -0.0281 -0.0226 -0.0172 -0.0336  0.0251\n",
      " -0.0132  0.0192  0.0294 -0.032  -0.0503 -0.0723  0.0549 -0.038  -0.1066\n",
      "  0.0163 -0.031  -0.1363 -0.0692  0.0489  0.0199  0.0202 -0.0974  0.006\n",
      " -0.0635 -0.0493 -0.0056  0.0816 -0.0035  0.0508  0.0136  0.0032  0.0138\n",
      "  0.0118 -0.128  -0.05   -0.0031  0.0319  0.0728  0.0146  0.0261  0.0103\n",
      "  0.0775 -0.0565 -0.1294 -0.0349 -0.071  -0.0146 -0.028   0.0389 -0.1014\n",
      "  0.0063  0.0443  0.0126 -0.019   0.0282  0.0518 -0.0181 -0.0344 -0.0266\n",
      " -0.0249 -0.0617  0.0782  0.0269 -0.1248 -0.0541  0.0223  0.0153 -0.0193\n",
      "  0.1342  0.0385 -0.0797  0.0391  0.008   0.0012 -0.0792 -0.0631  0.0533\n",
      "  0.0318  0.0418  0.0628 -0.0122  0.023   0.0404 -0.1073  0.0582 -0.0515\n",
      "  0.0398  0.0422  0.1488  0.0115 -0.0944 -0.0692 -0.0527 -0.0006 -0.0232\n",
      " -0.0801 -0.0769 -0.0228 -0.075   0.0213 -0.0236  0.07   -0.0457  0.1122\n",
      " -0.038  -0.0006 -0.072   0.0572 -0.047   0.1072  0.02   -0.0059 -0.0508\n",
      " -0.0503  0.0651  0.033  -0.0683 -0.0308 -0.0899  0.0051 -0.0005  0.0158\n",
      "  0.0654  0.0422  0.0342  0.0039  0.0424 -0.0858  0.0449  0.0326  0.0486\n",
      " -0.0362 -0.0102  0.0127 -0.0396 -0.0693  0.0017  0.0503  0.0346 -0.0282\n",
      "  0.1034 -0.0633 -0.0625 -0.064  -0.0167 -0.029   0.0435  0.0108  0.0052\n",
      "  0.0478  0.0084  0.0118 -0.0691 -0.0163 -0.0209  0.0421 -0.1104 -0.0824\n",
      "  0.1432  0.0926  0.0401 -0.0042 -0.0664  0.0409  0.0127  0.0326 -0.0474\n",
      "  0.0682 -0.0465  0.014 ]\n",
      "[-0.1296  0.0088  0.0494 -0.0142  0.0012  0.0351 -0.0267 -0.0571  0.0266\n",
      " -0.0473  0.093   0.0062  0.0447  0.1055  0.0297  0.005   0.07   -0.0086\n",
      "  0.1318  0.0027  0.0739  0.0137  0.0463  0.0398  0.0699  0.0155 -0.0165\n",
      "  0.0893 -0.0122  0.133   0.0542  0.0114 -0.0292 -0.009   0.0208 -0.0414\n",
      " -0.0494  0.0125  0.0639  0.0819  0.0129 -0.0675 -0.0734 -0.004  -0.0253\n",
      "  0.0577 -0.0936  0.0411  0.1271  0.0644 -0.051  -0.0355  0.0398 -0.0379\n",
      " -0.0268 -0.094   0.0738  0.0434 -0.04   -0.0182 -0.105   0.0166  0.0128\n",
      "  0.0628 -0.0668  0.0675 -0.0386 -0.0169  0.0306  0.0533  0.0036 -0.0319\n",
      " -0.0926 -0.0606 -0.1182 -0.0716  0.0714  0.0636 -0.0413 -0.0255 -0.0173\n",
      " -0.0335 -0.1446 -0.0286 -0.0433  0.094   0.0346  0.0199  0.0119 -0.0167\n",
      "  0.0466 -0.0615  0.0801  0.0493  0.02    0.0334  0.007   0.1235  0.0128\n",
      "  0.0018  0.0156 -0.0313  0.0106  0.0749 -0.0187 -0.1038 -0.0503 -0.0024\n",
      "  0.0229 -0.0299  0.0338  0.0851  0.0135 -0.0482 -0.0257 -0.1258 -0.0226\n",
      " -0.0225  0.0702  0.0527  0.0162  0.0875  0.0447  0.0366 -0.0116 -0.0141\n",
      "  0.0251 -0.0362 -0.0565  0.0093 -0.0581 -0.0082 -0.0383 -0.0426 -0.0893\n",
      " -0.0039  0.0314  0.0076  0.0351 -0.0386 -0.044   0.0394 -0.0359 -0.1032\n",
      "  0.0521 -0.0036 -0.1312 -0.054   0.058  -0.0484  0.045  -0.1239 -0.0329\n",
      " -0.0625 -0.0753 -0.0122  0.0244 -0.0238  0.0219  0.0407 -0.0041 -0.0178\n",
      " -0.0526 -0.0542  0.0165 -0.0055  0.0269  0.0189  0.0859 -0.0689 -0.0009\n",
      "  0.0788 -0.0154 -0.083   0.0003 -0.0776 -0.0275 -0.0782  0.0681 -0.099\n",
      " -0.0078  0.0775  0.032   0.0084 -0.0608  0.0897 -0.0358 -0.0254 -0.0159\n",
      " -0.0642 -0.0131  0.093   0.0717  0.08   -0.0155 -0.0216  0.0343 -0.0144\n",
      "  0.1485  0.0301 -0.0424  0.0268  0.0652  0.0138 -0.0175 -0.0527  0.0379\n",
      "  0.0455  0.0522  0.0205  0.0027  0.0971  0.036  -0.0684  0.0597 -0.0271\n",
      "  0.0354  0.0741  0.0666 -0.0186 -0.0577 -0.0901 -0.0467 -0.0196  0.0132\n",
      " -0.0119 -0.075   0.0314 -0.0576 -0.0904  0.0332  0.0229 -0.0265  0.0835\n",
      " -0.0576  0.0015 -0.0505  0.0822  0.0188  0.0727  0.0111 -0.0311  0.075\n",
      " -0.0301  0.0986 -0.004  -0.0443  0.0196 -0.0692 -0.0429  0.058  -0.016\n",
      "  0.1156  0.0053  0.001   0.0494  0.076  -0.0973 -0.0643  0.058   0.0365\n",
      " -0.0039  0.0023 -0.0406 -0.0119 -0.0528 -0.0229  0.1058  0.0338  0.0098\n",
      "  0.0684 -0.044  -0.0886 -0.0278 -0.0659 -0.081   0.078  -0.0877  0.0345\n",
      "  0.0395  0.0231 -0.0656 -0.0441 -0.0036 -0.0105 -0.0626 -0.0857 -0.0468\n",
      "  0.0545  0.126   0.0376 -0.019  -0.0463 -0.0921  0.0223 -0.0307 -0.0142\n",
      "  0.0899  0.0286  0.0361]\n"
     ]
    }
   ],
   "source": [
    "print(fasttext[\"google\"])\n",
    "print(fasttext[\"Google\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.66849655\n",
      "0.57657176\n"
     ]
    }
   ],
   "source": [
    "print(fasttext.similarity(\"google\", \"Google\"))\n",
    "print(fasttext.similarity(\"google\", \"search\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-02-08 15:19:20,145 : INFO : precomputing L2-norms of word weight vectors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun ==> [('Sun.The', 0.6877390146255493), ('Sun.I', 0.6652367115020752), ('SUn', 0.6346933841705322), ('Sat', 0.6030572652816772), ('-Sun', 0.6014525890350342), ('.Sun', 0.5839200019836426), ('Mon', 0.5815243721008301), ('Sun.', 0.5772802233695984), ('Fri', 0.5570669174194336), ('Sun4', 0.5426012873649597)]\n",
      "\n",
      "chess ==> [('Chess', 0.7488962411880493), ('non-chess', 0.7266454100608826), ('backgammon', 0.7085219025611877), ('chessplayers', 0.7080339193344116), ('chess.', 0.7011615037918091), ('chess-playing', 0.6884827613830566), ('chessboard', 0.6804870367050171), ('chess-related', 0.6718202829360962), ('chessmaster', 0.6695749759674072), ('chessplayer', 0.6695084571838379)]\n",
      "\n",
      "boy ==> [('girl', 0.8075398206710815), ('boy.He', 0.7046689987182617), ('boy-', 0.6979820728302002), ('boy.It', 0.6886506080627441), ('boys', 0.6847472190856934), ('boy.But', 0.6818395853042603), ('boy.This', 0.6790170669555664), ('boy.The', 0.6763201355934143), ('kid', 0.6720180511474609), ('boy.', 0.6698986291885376)]\n",
      "\n",
      "president ==> [('President', 0.7999043464660645), ('vice-president', 0.7897231578826904), ('president-elect', 0.7391069531440735), ('presdient', 0.7241677045822144), ('president.The', 0.7014009952545166), ('president.He', 0.7013550400733948), ('chairman', 0.6903572082519531), ('Vice-president', 0.6877903342247009), ('Vice-President', 0.6867066621780396), ('presidency', 0.6847482323646545)]\n",
      "\n",
      "hdmi ==> [('HDMI', 0.8415528535842896), ('s-video', 0.801682710647583), ('HDMi', 0.7873740196228027), ('svideo', 0.7723032236099243), ('vga', 0.7430283427238464), ('tv-out', 0.7332278490066528), ('displayport', 0.7309960722923279), ('Hdmi', 0.7258193492889404), ('dvi', 0.7243970036506653), ('spdif', 0.7193763256072998)]\n",
      "\n",
      "England ==> [('Engalnd', 0.735954761505127), ('England.The', 0.7215189933776855), ('Engand', 0.7197185754776001), ('England.It', 0.6980172395706177), ('Wales', 0.6957967281341553), ('Britain', 0.6942758560180664), ('England.He', 0.6861433982849121), ('England-', 0.6852438449859619), ('England.In', 0.669696569442749), ('England.As', 0.6680268049240112)]\n",
      "\n",
      "French ==> [('french', 0.737408459186554), ('.French', 0.7218425869941711), ('Frence', 0.7041172981262207), ('-French', 0.7010438442230225), ('Fench', 0.6839635372161865), ('French.The', 0.6802060604095459), ('non-French', 0.6774638891220093), ('Parisian', 0.670428991317749), ('theFrench', 0.6683896780014038), ('French.I', 0.647732138633728)]\n",
      "\n",
      "Google ==> [('Goolge', 0.794676661491394), ('it.Google', 0.7508618235588074), ('Gooogle', 0.7398059964179993), ('them.Google', 0.7207440137863159), ('Google.Google', 0.7200462818145752), ('.Google', 0.7153937816619873), ('Googl', 0.7104966640472412), ('you.Google', 0.7064957618713379), ('Google.It', 0.6990413665771484), ('Googles', 0.6810992956161499)]\n",
      "\n",
      "google ==> [('goolge', 0.799628496170044), ('gooogle', 0.7366892695426941), ('google.', 0.7345684766769409), ('goodle', 0.7199814915657043), ('google-search', 0.6932578682899475), ('seach', 0.6923413872718811), ('websearch', 0.6874836683273315), ('google-', 0.6821180582046509), ('Google', 0.6684964895248413), ('Gooogle', 0.6657105088233948)]\n",
      "\n",
      "keyboard ==> [('keybaord', 0.7966911196708679), ('keyboards', 0.7917160987854004), ('keybord', 0.7870093584060669), ('keyboad', 0.7858926057815552), ('key-board', 0.7715381979942322), ('keyboard.The', 0.7557199001312256), ('keyboard.', 0.7537243962287903), ('mini-keyboard', 0.7461830377578735), ('Keyboard', 0.7444150447845459), ('keboard', 0.7373706698417664)]\n",
      "\n",
      "mouse ==> [('mouses', 0.7489579319953918), ('mouse-', 0.7275699377059937), ('mouse.', 0.7206279039382935), ('mouse.The', 0.7091194987297058), ('mouse.I', 0.6861084699630737), ('mice', 0.6813828945159912), ('non-mouse', 0.6543790698051453), ('Mouse', 0.6431936621665955), ('mousing', 0.6345751285552979), ('mices', 0.6216095685958862)]\n",
      "\n",
      "plant ==> [('plants', 0.8447021245956421), ('plant.This', 0.7349259853363037), ('plant.It', 0.7328485250473022), ('plant.I', 0.7032804489135742), ('plant.The', 0.6871219873428345), ('plant.A', 0.6846728324890137), ('Plant', 0.6785468459129333), ('plant.But', 0.6782153844833374), ('plant.In', 0.6725714206695557), ('palnt', 0.6571705341339111)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "terms = \"Sun chess boy president hdmi England French Google google keyboard mouse plant\".split()\n",
    "\n",
    "for t in terms:\n",
    "    print(f\"{t} ==> {fasttext.most_similar(t)}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicio: Crea embeddings de oraciones y calcula similitud semántica\n",
    "\n",
    "1. Procesamos una oración completa\n",
    "2. Tokenizamos la frase\n",
    "3. Sustituimos cada token por su vector\n",
    "4. Calculamos el promedio de los vectores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from nltk import word_tokenize\n",
    "\n",
    "def embed(text: str) -> np.array:\n",
    "    \"\"\"Crea el embedding de un documento combinando los vectores de palabras de fastText\"\"\"\n",
    "    tokens = word_tokenize(text)\n",
    "    word_embeddings = [fasttext[token] for token in tokens]\n",
    "    doc_embedding = np.mean(word_embeddings, axis=0) \n",
    "    return doc_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "(300,)\n"
     ]
    }
   ],
   "source": [
    "embedding = embed(\"This is just an example.\")\n",
    "print(type(embedding))\n",
    "print(embedding.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [\n",
    "    \"Pizza and pasta are my favorite food!\", \n",
    "    \"I hate football\", \n",
    "    \"I have a blue Toyota Corolla.\", \n",
    "    \"I enjoy playing tennis.\",\n",
    "    \"I love ice-cream :-)\",\n",
    "    \"Green colorless ideas sleep furiously.\",\n",
    "    \"Relations between Washington and Beijing have been tense for years.\",\n",
    "    \"Chinese officials have criticized the United States.\",\n",
    "    \"U.S. Women's Team Qualifies for Olympic Soccer Tournament\"\n",
    "]\n",
    "\n",
    "doc_embeddings = [embed(text) for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def knn_search(text: str, k: int = 3) -> None:\n",
    "    \"\"\"Búsqueda basada en vecinos cercanos (KNN)\"\"\"\n",
    "    text_vec = embed(text)\n",
    "    score = np.sum(text_vec * doc_embeddings, axis=1) / np.linalg.norm(doc_embeddings, axis=1)\n",
    "    topk_idx = np.argsort(score)[::-1][:k]\n",
    "    for idx in topk_idx:\n",
    "        print(f\"{texts[idx]} -> {score[idx]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I have a blue Toyota Corolla. -> 1.0283704996109009\n",
      "I enjoy playing tennis. -> 0.8112152814865112\n",
      "I hate football -> 0.8047472834587097\n"
     ]
    }
   ],
   "source": [
    "knn_search(\"I own a reliable Japanese car\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I hate football -> 1.1839185953140259\n",
      "I enjoy playing tennis. -> 1.1252000331878662\n",
      "I have a blue Toyota Corolla. -> 0.9867241978645325\n"
     ]
    }
   ],
   "source": [
    "knn_search(\"I enjoy sports\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(text: str) -> None:\n",
    "    \"\"\"Calcula la similitud semántica entre un documento y una colección de documentos\"\"\"\n",
    "    text_vec = embed(text)\n",
    "    for i, embedding in enumerate(doc_embeddings):\n",
    "        similarity = np.dot(text_vec, embedding) / (np.linalg.norm(text_vec) * np.linalg.norm(embedding))\n",
    "        print(f\"{texts[i]} -> {similarity}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pizza and pasta are my favorite food! -> 0.7369285821914673\n",
      "I hate football -> 0.8678369522094727\n",
      "I have a blue Toyota Corolla. -> 0.7520216703414917\n",
      "I enjoy playing tennis. -> 0.793563187122345\n",
      "I love ice-cream :-) -> 0.651222288608551\n",
      "Green colorless ideas sleep furiously. -> 0.47906479239463806\n",
      "Relations between Washington and Beijing have been tense for years. -> 0.5077171921730042\n",
      "Chinese officials have criticized the United States. -> 0.5151601433753967\n",
      "U.S. Women's Team Qualifies for Olympic Soccer Tournament -> 0.28166434168815613\n"
     ]
    }
   ],
   "source": [
    "cosine_similarity(\"I like Italian food\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pizza and pasta are my favorite food! -> 0.42676427960395813\n",
      "I hate football -> 0.2961423695087433\n",
      "I have a blue Toyota Corolla. -> 0.4360668957233429\n",
      "I enjoy playing tennis. -> 0.3851541578769684\n",
      "I love ice-cream :-) -> 0.32622864842414856\n",
      "Green colorless ideas sleep furiously. -> 0.30096158385276794\n",
      "Relations between Washington and Beijing have been tense for years. -> 0.4897002577781677\n",
      "Chinese officials have criticized the United States. -> 0.5181446075439453\n",
      "U.S. Women's Team Qualifies for Olympic Soccer Tournament -> 0.5080596208572388\n"
     ]
    }
   ],
   "source": [
    "cosine_similarity(\"Female sport in America\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
